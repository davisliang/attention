{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxnet.gluon import nn, Block\n",
    "\n",
    "def one_hots(numerical_list, vocab_size):\n",
    "    result = nd.zeros((len(numerical_list), vocab_size), ctx=ctx)\n",
    "    for i, idx in enumerate(numerical_list):\n",
    "        result[i, idx] = 1.0\n",
    "    return result\n",
    "\n",
    "def textify(embedding):\n",
    "    result = \"\"\n",
    "    indices = nd.argmax(embedding, axis=1).asnumpy()\n",
    "    for idx in indices:\n",
    "        result += character_list[int(idx)]\n",
    "    return result\n",
    "\n",
    "def load_time_machine(seq_length=64, batch_size=1):\n",
    "    # loading dataset\n",
    "    path = \"../../data/timemachine.txt\"\n",
    "    with open(path) as f:\n",
    "        time_machine = f.read()\n",
    "    time_machine = time_machine[:-38083] #hardcoded to remove crap\n",
    "    character_dict, vocab_size = get_char_dict(time_machine)\n",
    "    \n",
    "    time_numerical = [character_dict[char] for char in time_machine]\n",
    "    # -1 here so we have enough characters for labels later\n",
    "    num_samples = (len(time_numerical) - 1) // seq_length\n",
    "    dataset = one_hots(time_numerical[:seq_length*num_samples],vocab_size).reshape((num_samples, seq_length, vocab_size))\n",
    "    num_batches = len(dataset) // batch_size\n",
    "    train_data = dataset[:num_batches*batch_size].reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "    \n",
    "    # swap batch_size and seq_length axis to make later access easier\n",
    "    train_data = nd.swapaxes(train_data, 0, 1)\n",
    "    train_data = nd.swapaxes(train_data, 1, 2)\n",
    "    print('Shape of data set: ', train_data.shape)\n",
    "    \n",
    "    labels = one_hots(time_numerical[1:seq_length*num_samples+1], vocab_size)\n",
    "    train_label = labels.reshape((batch_size, num_batches, seq_length, vocab_size))\n",
    "    train_label = nd.swapaxes(train_label, 0, 1)\n",
    "    train_label = nd.swapaxes(train_label, 1, 2)\n",
    "    print('Shape of label set: ', train_label.shape)\n",
    "    \n",
    "    return train_data, train_label\n",
    "    \n",
    "\n",
    "def get_char_dict(data):\n",
    "    # get character dictionary\n",
    "    character_list = list(set(data))\n",
    "    vocab_size = len(character_list)\n",
    "    # get the character dictionary\n",
    "    character_dict = {}\n",
    "    for e, char in enumerate(character_list):\n",
    "        character_dict[char] = e\n",
    "    return character_dict, vocab_size\n",
    "\n",
    "def rnn_helper(num_hidden, vocab_size): \n",
    "    num_inputs = vocab_size\n",
    "    num_outputs = vocab_size\n",
    "    Wxh = nd.random_normal(shape=(num_inputs,num_hidden), ctx=ctx) * .01\n",
    "    Whh = nd.random_normal(shape=(num_hidden,num_hidden), ctx=ctx) * .01\n",
    "    bh = nd.random_normal(shape=num_hidden, ctx=ctx) * .01\n",
    "    Why = nd.random_normal(shape=(num_hidden,num_outputs), ctx=ctx) * .01\n",
    "    by = nd.random_normal(shape=num_outputs, ctx=ctx) * .01\n",
    "    params = [Wxh, Whh, bh, Why, by]\n",
    "\n",
    "    for param in params:\n",
    "        param.attach_grad()\n",
    "    return params\n",
    " \n",
    "def softmax(y_linear):\n",
    "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
    "    partition = nd.nansum(exp, axis=0, exclude=True)\n",
    "    return exp / partition\n",
    "\n",
    "def encoder(steps, input_data, num_hidden, vocab_size, state, params):\n",
    "    Wxh, Whh, bh, Why, by = params\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for i in range(input_data.shape[0]):\n",
    "        h_linear = nd.dot(input_data[i], Wxh) + nd.dot(h, Whh) + bh\n",
    "        h = nd.tanh(h_linear)\n",
    "        yhat_linear = nd.dot(h, Why) + by\n",
    "        yhat = softmax(yhat_linear) \n",
    "        outputs.append(nd.expand_dims(yhat[0],axis=1))\n",
    "    return (outputs, h)\n",
    "\n",
    "def attention(decoder_hidden_t, encoder_output):\n",
    "    if(decoder_hidden_t.shape[1] != encoder_output.shape[0]):\n",
    "        encoder_output = encoder_output.T\n",
    "    return nd.dot(softmax(nd.dot(decoder_hidden_t, encoder_output)) , encoder_output.T)\n",
    " \n",
    "def decoder(steps, encoder_outputs, state, num_hidden, vocab_size, params):\n",
    "    Wxh, Whh, bh, Why, by = params\n",
    "    outputs = []\n",
    "    h = state\n",
    "    for i in range(steps):\n",
    "        h=nd.reshape(h,(1,h.size))\n",
    "        yhat = softmax(nd.dot(nd.tanh(nd.dot(attention(h, encoder_outputs), Wxh) + nd.dot(h, Whh) + bh), Why) + by) \n",
    "        outputs.append(yhat[0])\n",
    "    return (outputs, h)    \n",
    "    \n",
    "def SGD(params, lr):    \n",
    "    for param in params:\n",
    "        param[:] = param - lr * param.grad\n",
    "        \n",
    "\n",
    "def cross_entropy(yhat, y):\n",
    "    return - nd.mean(nd.sum(y * nd.log(yhat), axis=0, exclude=True))\n",
    "\n",
    "\n",
    "def average_ce_loss(outputs, labels):\n",
    "    assert(len(outputs) == len(labels))\n",
    "    total_loss = 0.\n",
    "    for (output, label) in zip(outputs,labels):\n",
    "        total_loss = total_loss + cross_entropy(output, label)\n",
    "    return total_loss / len(outputs)\n",
    "\n",
    "class decoder_layer(Block):\n",
    "    def __init__(self, steps, num_hidden, vocab_size):\n",
    "        super(encoder_layer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layer meta information\n",
    "            self.steps = steps\n",
    "            self.num_hidden = num_hidden\n",
    "            self.vocab_size = vocab_size\n",
    "            self.num_inputs = vocab_size\n",
    "            self.num_outputs = vocab_size\n",
    "            \n",
    "            # initialize layer RNN parameters\n",
    "            self.Wxh = self.params.get('d_Wxh', shape=(self.num_inputs,num_hidden), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.Whh = self.params.get('d_Whh', shape=(num_hidden,num_hidden), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.bh = self.params.get('d_bh', shape=num_hidden)\n",
    "            self.Why = self.params.get('d_Why', shape=(num_hidden,self.num_outputs), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.by = self.params.get('d_by', shape=self.num_outputs)\n",
    "    def forward(self,input_data,hidden_state):\n",
    "        with input_data.context:\n",
    "            outputs = []\n",
    "            h=state\n",
    "            for i in range(input_data.shape[0]):\n",
    "                h_linear = nd.dot(attention(input_data[i]), Wxh) + nd.dot(h, Whh) + bh\n",
    "                h = nd.tanh(h_linear)\n",
    "                yhat_linear = nd.dot(h, Why) + by\n",
    "                yhat = softmax(yhat_linear) \n",
    "                outputs.append(nd.expand_dims(yhat[0],axis=1))\n",
    "            return (outputs, h)\n",
    "\n",
    "class encoder_layer(Block):\n",
    "    def __init__(self, steps, num_hidden, vocab_size, **kwargs):\n",
    "        super(encoder_layer, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            # layer meta information\n",
    "            self.steps = steps\n",
    "            self.num_hidden = num_hidden\n",
    "            self.vocab_size = vocab_size\n",
    "            self.num_inputs = vocab_size\n",
    "            self.num_outputs = vocab_size\n",
    "            \n",
    "            # initialize layer RNN parameters\n",
    "            self.Wxh = self.params.get('e_Wxh', shape=(self.num_inputs,num_hidden), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.Whh = self.params.get('e_Whh', shape=(num_hidden,num_hidden), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.bh = self.params.get('e_bh', shape=num_hidden)\n",
    "            self.Why = self.params.get('e_Why', shape=(num_hidden,self.num_outputs), init=mx.init.Xavier(magnitude=2.24))\n",
    "            self.by = self.params.get('e_by', shape=self.num_outputs)\n",
    "    def forward(self,input_data, hidden_state):\n",
    "        with input_data.context:\n",
    "            outputs = []\n",
    "            h=state\n",
    "            for i in range(input_data.shape[0]):\n",
    "                h_linear = nd.dot(input_data[i], Wxh) + nd.dot(h, Whh) + bh\n",
    "                h = nd.tanh(h_linear)\n",
    "                yhat_linear = nd.dot(h, Why) + by\n",
    "                yhat = softmax(yhat_linear) \n",
    "                outputs.append(nd.expand_dims(yhat[0],axis=1))\n",
    "            return (outputs, h)\n",
    "def list_to_nd_array(list_of_nd_arrays):\n",
    "    return nd.concat(*list_of_nd_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape of data set: ', (2600L, 64L, 1L, 88L))\n",
      "('Shape of label set: ', (2600L, 64L, 1L, 88L))\n"
     ]
    }
   ],
   "source": [
    "# context usage\n",
    "ctx = mx.cpu()\n",
    "data, labels = load_time_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = encoder_layer(64,100,88)\n",
    "encoder.collect_params().initialize(ctx=mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_hidden = 88\n",
    "steps = 64\n",
    "learning_rate = 0.01\n",
    "vocab_size = 88\n",
    "\n",
    "decoder_params = rnn_helper(num_hidden, vocab_size)\n",
    "encoder_params = rnn_helper(num_hidden, vocab_size)\n",
    "params = decoder_params + encoder_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ -3.21308704e-07   3.03495455e-07  -8.38009726e-07 ...,   4.24107384e-07\n",
      "   -2.24400040e-07  -2.65756256e-07]\n",
      " [ -3.15429304e-07   2.97942250e-07  -8.22675759e-07 ...,   4.16347206e-07\n",
      "   -2.20293998e-07  -2.60893415e-07]\n",
      " [ -3.20355952e-07   3.02595680e-07  -8.35525043e-07 ...,   4.22849894e-07\n",
      "   -2.23734787e-07  -2.64968435e-07]\n",
      " ..., \n",
      " [ -3.18099296e-07   3.00464166e-07  -8.29639362e-07 ...,   4.19871270e-07\n",
      "   -2.22158860e-07  -2.63101867e-07]\n",
      " [ -3.22294341e-07   3.04426663e-07  -8.40580526e-07 ...,   4.25408530e-07\n",
      "   -2.25088527e-07  -2.66571590e-07]\n",
      " [ -3.16986871e-07   2.99413358e-07  -8.26738244e-07 ...,   4.18402891e-07\n",
      "   -2.21381882e-07  -2.62181970e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -3.45790568e-07  -1.52924062e-09  -9.32157036e-07 ...,   3.64817339e-07\n",
      "   -1.34816304e-07   5.24288453e-08]\n",
      " [ -3.39537479e-07  -1.50158841e-09  -9.15300404e-07 ...,   3.58220120e-07\n",
      "   -1.32378361e-07   5.14807184e-08]\n",
      " [ -3.44817806e-07  -1.52493129e-09  -9.29534508e-07 ...,   3.63790804e-07\n",
      "   -1.34436988e-07   5.22813011e-08]\n",
      " ..., \n",
      " [ -3.42403041e-07  -1.51424995e-09  -9.23025254e-07 ...,   3.61243366e-07\n",
      "   -1.33495561e-07   5.19152010e-08]\n",
      " [ -3.46906802e-07  -1.53416835e-09  -9.35166099e-07 ...,   3.65994850e-07\n",
      "   -1.35251440e-07   5.25980468e-08]\n",
      " [ -3.41195204e-07  -1.50891422e-09  -9.19768922e-07 ...,   3.59968908e-07\n",
      "   -1.33024656e-07   5.17320657e-08]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -2.97025281e-07  -2.24795144e-07  -5.80595099e-07 ...,   4.00367156e-07\n",
      "    3.05312369e-07  -8.86766145e-08]\n",
      " [ -2.91722984e-07  -2.20782169e-07  -5.70230725e-07 ...,   3.93220006e-07\n",
      "    2.99862137e-07  -8.70935608e-08]\n",
      " [ -2.96202103e-07  -2.24172140e-07  -5.78986146e-07 ...,   3.99257544e-07\n",
      "    3.04466226e-07  -8.84309372e-08]\n",
      " ..., \n",
      " [ -2.94111828e-07  -2.22590245e-07  -5.74900582e-07 ...,   3.96440470e-07\n",
      "    3.02317716e-07  -8.78069315e-08]\n",
      " [ -2.98015379e-07  -2.25544397e-07  -5.82530447e-07 ...,   4.01701641e-07\n",
      "    3.06330008e-07  -8.89722216e-08]\n",
      " [ -2.93128238e-07  -2.21845781e-07  -5.72977740e-07 ...,   3.95114341e-07\n",
      "    3.01306613e-07  -8.75131434e-08]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -3.81656719e-07   2.83348793e-08  -1.10961093e-06 ...,   3.00556792e-07\n",
      "    1.10178732e-07   9.57745172e-09]\n",
      " [ -3.74730377e-07   2.78206098e-08  -1.08947347e-06 ...,   2.95102382e-07\n",
      "    1.08179236e-07   9.40362455e-09]\n",
      " [ -3.80550290e-07   2.82527530e-08  -1.10639451e-06 ...,   2.99685723e-07\n",
      "    1.09859407e-07   9.54969614e-09]\n",
      " ..., \n",
      " [ -3.77914631e-07   2.80570553e-08  -1.09873122e-06 ...,   2.97610086e-07\n",
      "    1.09098480e-07   9.48354639e-09]\n",
      " [ -3.82905483e-07   2.84275750e-08  -1.11324118e-06 ...,   3.01540268e-07\n",
      "    1.10539219e-07   9.60877600e-09]\n",
      " [ -3.76584183e-07   2.79583059e-08  -1.09486325e-06 ...,   2.96562177e-07\n",
      "    1.08714403e-07   9.45015977e-09]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -3.01327646e-07   7.45284510e-08  -7.02849547e-07 ...,   4.24820996e-07\n",
      "    2.43557366e-07   1.80404207e-08]\n",
      " [ -2.95842312e-07   7.31717478e-08  -6.90055458e-07 ...,   4.17087620e-07\n",
      "    2.39123750e-07   1.77119350e-08]\n",
      " [ -3.00459021e-07   7.43136184e-08  -7.00823989e-07 ...,   4.23596504e-07\n",
      "    2.42855378e-07   1.79883592e-08]\n",
      " ..., \n",
      " [ -2.98279986e-07   7.37746646e-08  -6.95741335e-07 ...,   4.20524259e-07\n",
      "    2.41094000e-07   1.78579178e-08]\n",
      " [ -3.02244757e-07   7.47552917e-08  -7.04989247e-07 ...,   4.26114042e-07\n",
      "    2.44298690e-07   1.80952746e-08]\n",
      " [ -2.97309356e-07   7.35345722e-08  -6.93477148e-07 ...,   4.19155811e-07\n",
      "    2.40309390e-07   1.77997883e-08]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -7.66801733e-08   2.76724876e-07  -4.68613706e-07 ...,   3.01264123e-07\n",
      "   -5.12331724e-07   2.41092209e-07]\n",
      " [ -7.53053868e-08   2.71763469e-07  -4.60211851e-07 ...,   2.95862890e-07\n",
      "   -5.03146225e-07   2.36769807e-07]\n",
      " [ -7.64226229e-08   2.75795486e-07  -4.67039996e-07 ...,   3.00252424e-07\n",
      "   -5.10611301e-07   2.40282674e-07]\n",
      " ..., \n",
      " [ -7.59231042e-08   2.73993180e-07  -4.63987959e-07 ...,   2.98290303e-07\n",
      "   -5.07274308e-07   2.38712374e-07]\n",
      " [ -7.69334605e-08   2.77638975e-07  -4.70161666e-07 ...,   3.02259338e-07\n",
      "   -5.14024293e-07   2.41888642e-07]\n",
      " [ -7.56667191e-08   2.73067599e-07  -4.62420388e-07 ...,   2.97282611e-07\n",
      "   -5.05560649e-07   2.37905923e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -2.15885478e-07  -6.81263188e-08  -6.49354206e-07 ...,   1.30157019e-07\n",
      "   -7.11318151e-08  -2.56739924e-07]\n",
      " [ -2.12047894e-07  -6.69153053e-08  -6.37811297e-07 ...,   1.27843336e-07\n",
      "   -6.98673688e-08  -2.52176136e-07]\n",
      " [ -2.15264762e-07  -6.79304506e-08  -6.47487070e-07 ...,   1.29782805e-07\n",
      "   -7.09272925e-08  -2.56001726e-07]\n",
      " ..., \n",
      " [ -2.13757730e-07  -6.74548772e-08  -6.42954149e-07 ...,   1.28874234e-07\n",
      "   -7.04307723e-08  -2.54209539e-07]\n",
      " [ -2.16615788e-07  -6.83567833e-08  -6.51550863e-07 ...,   1.30597343e-07\n",
      "   -7.13724617e-08  -2.57608491e-07]\n",
      " [ -2.13019817e-07  -6.72220466e-08  -6.40734584e-07 ...,   1.28429363e-07\n",
      "   -7.01876317e-08  -2.53331962e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -9.23981318e-08  -4.87036864e-08  -3.56380013e-07 ...,   1.29862983e-07\n",
      "   -2.25380475e-07   4.25052974e-07]\n",
      " [ -9.07584337e-08  -4.78394568e-08  -3.50055927e-07 ...,   1.27558451e-07\n",
      "   -2.21381043e-07   4.17510250e-07]\n",
      " [ -9.21043650e-08  -4.85488769e-08  -3.55247124e-07 ...,   1.29450115e-07\n",
      "   -2.24664063e-07   4.23701806e-07]\n",
      " ..., \n",
      " [ -9.14963891e-08  -4.82283902e-08  -3.52901793e-07 ...,   1.28595531e-07\n",
      "   -2.23180948e-07   4.20904684e-07]\n",
      " [ -9.27085821e-08  -4.88673635e-08  -3.57577477e-07 ...,   1.30299313e-07\n",
      "   -2.26137814e-07   4.26481193e-07]\n",
      " [ -9.11919429e-08  -4.80678892e-08  -3.51727550e-07 ...,   1.28167684e-07\n",
      "   -2.22438274e-07   4.19504204e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -6.49824088e-08   1.53906555e-07  -3.83389562e-07 ...,   3.01922114e-07\n",
      "   -8.64899960e-07   4.00456940e-07]\n",
      " [ -6.38200603e-08   1.51153671e-07  -3.76531830e-07 ...,   2.96521819e-07\n",
      "   -8.49429682e-07   3.93294300e-07]\n",
      " [ -6.47393961e-08   1.53331001e-07  -3.81955715e-07 ...,   3.00793005e-07\n",
      "   -8.61665228e-07   3.98959628e-07]\n",
      " ..., \n",
      " [ -6.43376765e-08   1.52379542e-07  -3.79585600e-07 ...,   2.98926579e-07\n",
      "   -8.56318820e-07   3.96483870e-07]\n",
      " [ -6.51775878e-08   1.54368905e-07  -3.84541210e-07 ...,   3.02829108e-07\n",
      "   -8.67497988e-07   4.01660031e-07]\n",
      " [ -6.40957296e-08   1.51806560e-07  -3.78158234e-07 ...,   2.97802558e-07\n",
      "   -8.53098584e-07   3.94993378e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[  2.04301386e-07   5.40666640e-08  -2.47483285e-07 ...,   3.75646096e-07\n",
      "   -7.61314311e-07   6.44381657e-07]\n",
      " [  2.00679096e-07   5.31080389e-08  -2.43095428e-07 ...,   3.68985809e-07\n",
      "   -7.47816728e-07   6.32956869e-07]\n",
      " [  2.03627863e-07   5.38883995e-08  -2.46667469e-07 ...,   3.74407790e-07\n",
      "   -7.58804958e-07   6.42257419e-07]\n",
      " ..., \n",
      " [  2.02313217e-07   5.35404858e-08  -2.45074887e-07 ...,   3.71990410e-07\n",
      "   -7.53906022e-07   6.38111032e-07]\n",
      " [  2.04989817e-07   5.42488436e-08  -2.48317264e-07 ...,   3.76911885e-07\n",
      "   -7.63880280e-07   6.46553246e-07]\n",
      " [  2.01632446e-07   5.33603774e-08  -2.44250231e-07 ...,   3.70738775e-07\n",
      "   -7.51369100e-07   6.35963602e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[  2.89356620e-07  -2.04855056e-07   5.20796206e-09 ...,   3.07789975e-07\n",
      "   -6.98897168e-07   6.37922255e-07]\n",
      " [  2.84188758e-07  -2.01196386e-07   5.11498577e-09 ...,   3.02293188e-07\n",
      "   -6.86414978e-07   6.26529072e-07]\n",
      " [  2.88353277e-07  -2.04144783e-07   5.18994803e-09 ...,   3.06722967e-07\n",
      "   -6.96473819e-07   6.35710308e-07]\n",
      " ..., \n",
      " [  2.86520759e-07  -2.02847332e-07   5.15697174e-09 ...,   3.04773721e-07\n",
      "   -6.92047536e-07   6.31670162e-07]\n",
      " [  2.90352148e-07  -2.05559786e-07   5.22590149e-09 ...,   3.08849224e-07\n",
      "   -7.01301815e-07   6.40116923e-07]\n",
      " [  2.85463926e-07  -2.02099187e-07   5.13801623e-09 ...,   3.03649557e-07\n",
      "   -6.89494982e-07   6.29340320e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -5.03828971e-07   2.39876726e-07  -8.21780191e-07 ...,   4.65967076e-07\n",
      "   -3.77975965e-07  -2.33201174e-08]\n",
      " [ -4.94665755e-07   2.35514051e-07  -8.06834407e-07 ...,   4.57492632e-07\n",
      "   -3.71101720e-07  -2.28959749e-08]\n",
      " [ -5.02192847e-07   2.39097801e-07  -8.19111563e-07 ...,   4.64453933e-07\n",
      "   -3.76748659e-07  -2.32443611e-08]\n",
      " ..., \n",
      " [ -4.98775591e-07   2.37470843e-07  -8.13538009e-07 ...,   4.61293610e-07\n",
      "   -3.74184907e-07  -2.30862174e-08]\n",
      " [ -5.05390403e-07   2.40620182e-07  -8.24327003e-07 ...,   4.67411212e-07\n",
      "   -3.79147366e-07  -2.33923743e-08]\n",
      " [ -4.97102974e-07   2.36674438e-07  -8.10809581e-07 ...,   4.59746616e-07\n",
      "   -3.72929975e-07  -2.30087878e-08]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -5.49169272e-07  -1.40199163e-07  -8.39344523e-07 ...,   2.20423587e-07\n",
      "   -5.60809326e-07  -1.04644307e-07]\n",
      " [ -5.39285907e-07  -1.37675926e-07  -8.24238725e-07 ...,   2.16456669e-07\n",
      "   -5.50716379e-07  -1.02761000e-07]\n",
      " [ -5.47418892e-07  -1.39752245e-07  -8.36669074e-07 ...,   2.19721073e-07\n",
      "   -5.59021771e-07  -1.04310743e-07]\n",
      " ..., \n",
      " [ -5.43779549e-07  -1.38823182e-07  -8.31107116e-07 ...,   2.18260325e-07\n",
      "   -5.55305519e-07  -1.03617332e-07]\n",
      " [ -5.51015717e-07  -1.40670565e-07  -8.42166799e-07 ...,   2.21164882e-07\n",
      "   -5.62695050e-07  -1.04996190e-07]\n",
      " [ -5.41733982e-07  -1.38300905e-07  -8.27980330e-07 ...,   2.17439307e-07\n",
      "   -5.53216239e-07  -1.03227549e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -2.27591727e-07   1.76203159e-08  -5.89881267e-07 ...,   3.02587864e-07\n",
      "   -2.88133549e-07   4.09962837e-08]\n",
      " [ -2.23487817e-07   1.73025612e-08  -5.79244386e-07 ...,   2.97131550e-07\n",
      "   -2.82938004e-07   4.02570279e-08]\n",
      " [ -2.26918630e-07   1.75681762e-08  -5.88136345e-07 ...,   3.01692950e-07\n",
      "   -2.87281381e-07   4.08750402e-08]\n",
      " ..., \n",
      " [ -2.25327028e-07   1.74449362e-08  -5.84011332e-07 ...,   2.99576755e-07\n",
      "   -2.85266481e-07   4.05883149e-08]\n",
      " [ -2.28299754e-07   1.76750827e-08  -5.91716059e-07 ...,   3.03529077e-07\n",
      "   -2.89029828e-07   4.11237941e-08]\n",
      " [ -2.24540884e-07   1.73840640e-08  -5.81973836e-07 ...,   2.98531688e-07\n",
      "   -2.84271238e-07   4.04467251e-08]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -3.48409316e-07  -7.20566220e-08  -8.64139565e-07 ...,   2.14654065e-07\n",
      "   -3.55536088e-07  -1.04348906e-07]\n",
      " [ -3.42076589e-07  -7.07468928e-08  -8.48432819e-07 ...,   2.10752432e-07\n",
      "   -3.49073787e-07  -1.02452297e-07]\n",
      " [ -3.47300869e-07  -7.18273583e-08  -8.61390731e-07 ...,   2.13971106e-07\n",
      "   -3.54404989e-07  -1.04016983e-07]\n",
      " ..., \n",
      " [ -3.44925780e-07  -7.13361530e-08  -8.55499593e-07 ...,   2.12507814e-07\n",
      "   -3.51981214e-07  -1.03305624e-07]\n",
      " [ -3.49450687e-07  -7.22719875e-08  -8.66722473e-07 ...,   2.15295614e-07\n",
      "   -3.56598861e-07  -1.04660813e-07]\n",
      " [ -3.43649106e-07  -7.10721082e-08  -8.52332903e-07 ...,   2.11721286e-07\n",
      "   -3.50678306e-07  -1.02923252e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -4.40220674e-07  -2.29818067e-08  -8.37207324e-07 ...,   3.82136903e-07\n",
      "   -2.40109841e-07  -1.91960055e-07]\n",
      " [ -4.32286214e-07  -2.25675709e-08  -8.22118011e-07 ...,   3.75249471e-07\n",
      "   -2.35782153e-07  -1.88500280e-07]\n",
      " [ -4.38791034e-07  -2.29071411e-08  -8.34488390e-07 ...,   3.80895784e-07\n",
      "   -2.39330063e-07  -1.91336710e-07]\n",
      " ..., \n",
      " [ -4.35779583e-07  -2.27499335e-08  -8.28761358e-07 ...,   3.78281811e-07\n",
      "   -2.37687473e-07  -1.90023528e-07]\n",
      " [ -4.41641419e-07  -2.30559714e-08  -8.39909319e-07 ...,   3.83370150e-07\n",
      "   -2.40884702e-07  -1.92579648e-07]\n",
      " [ -4.34307253e-07  -2.26730847e-08  -8.25961138e-07 ...,   3.77003744e-07\n",
      "   -2.36884460e-07  -1.89381524e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n",
      "\n",
      "[[ -4.24690825e-08  -2.50279868e-07  -5.37371875e-07 ...,   1.47045313e-07\n",
      "   -4.55429642e-07   5.48858964e-07]\n",
      " [ -4.17093169e-08  -2.45802028e-07  -5.27757834e-07 ...,   1.44414457e-07\n",
      "   -4.47281423e-07   5.39039092e-07]\n",
      " [ -4.23328572e-08  -2.49476841e-07  -5.35647871e-07 ...,   1.46573512e-07\n",
      "   -4.53968141e-07   5.47097841e-07]\n",
      " ..., \n",
      " [ -4.20557882e-08  -2.47843957e-07  -5.32141939e-07 ...,   1.45614166e-07\n",
      "   -4.50997021e-07   5.43517217e-07]\n",
      " [ -4.26197744e-08  -2.51167876e-07  -5.39278687e-07 ...,   1.47567022e-07\n",
      "   -4.57045275e-07   5.50806362e-07]\n",
      " [ -4.19025312e-08  -2.46940857e-07  -5.30202726e-07 ...,   1.45083490e-07\n",
      "   -4.49353365e-07   5.41536451e-07]]\n",
      "<NDArray 88x88 @cpu(0)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-189c1143e0c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_ce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m88\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mprint\u001b[0m \u001b[0mdecoder_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/davis/anaconda2/lib/python2.7/site-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;34m\"\"\"Returns a string representation of the array.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mshape_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         return '\\n%s\\n<%s %s @%s>' % (str(self.asnumpy()),\n\u001b[0m\u001b[1;32m    183\u001b[0m                                       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                                       shape_info, self.context)\n",
      "\u001b[0;32m/home/davis/anaconda2/lib/python2.7/site-packages/mxnet/ndarray/ndarray.pyc\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1794\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for i in range(data.shape[0]):\n",
    "        with autograd.record():\n",
    "            output_encoder,hidden_encoder=encoder(steps, data[i], num_hidden, int(data.shape[3]), nd.zeros(num_hidden),encoder_params)\n",
    "            out_enc = list_to_nd_array(output_encoder)\n",
    "            output_decoder, hidden_state = decoder(steps,out_enc,nd.zeros(num_hidden),num_hidden,int(data.shape[3]),decoder_params)\n",
    "            loss = average_ce_loss(output_decoder, nd.reshape(labels[i],(64,88))) \n",
    "        loss.backward()\n",
    "        print decoder_params[0].grad\n",
    "        SGD(params, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_encoder,hidden_encoder=encoder(steps, data[i], num_hidden, int(data.shape[3]), nd.zeros(num_hidden),encoder_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoder_hidden_t' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-a1ae8757c2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'decoder_hidden_t' is not defined"
     ]
    }
   ],
   "source": [
    "attention(decoder_hidden_t, encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_out=output_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dec_hid=output_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention(decoder_hidden_t, encoder_output):\n",
    "    #if(decoder_hidden_t.shape[0]!=1):\n",
    "    #    decoder_hidden_t = nd.expand_dims(decoder_hidden_t,axis=0)\n",
    "    return nd.dot(softmax(nd.dot(decoder_hidden_t, encoder_output.T)) , encoder_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-573581a75332>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_hid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-d41dfa4449a5>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(decoder_hidden_t, encoder_output)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdecoder_hidden_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "attention(dec_hid,enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=nd.ones(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5L"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.  1.  1.  1.  1.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 1.  1.  1.  1.  1.]]\n",
       "<NDArray 1x5 @cpu(0)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nd.reshape(x,(1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[ 1.  1.  1.  1.  1.]\n",
       "<NDArray 5 @cpu(0)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
